{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09da1bd",
   "metadata": {},
   "source": [
    "## Part I: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e810a",
   "metadata": {},
   "source": [
    "### Task 1: Theory Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb33801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "theory_nb = {\n",
    "    \"1. Core Assumption\": \"Naive Bayes assumes that all features are conditionally independent given the class label. This means the presence of one feature does not affect the presence of another.\",\n",
    "    \"2. Types of Naive Bayes\":\n",
    "        \"- GaussianNB: Used for continuous features assuming normal distribution.\\n\"\n",
    "        \"- MultinomialNB: Best for discrete counts (e.g., word frequencies in text).\\n\"\n",
    "        \"- BernoulliNB: For binary/boolean features (e.g., word present or not).\",\n",
    "    \"3. High-dimensional suitability\":\n",
    "        \"Naive Bayes works well with high-dimensional data like text because it simplifies calculations using independence assumption, making it computationally efficient even with many features.\"\n",
    "}\n",
    "\n",
    "for q, a in theory_nb.items():\n",
    "    display(md(f\"**{q}**  \\n{a}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d1aa6",
   "metadata": {},
   "source": [
    "### Task 2: Spam Detection using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_table(url, header=None, names=['label', 'message'])\n",
    "df['label_num'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label_num'], test_size=0.2, random_state=42)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738bf84f",
   "metadata": {},
   "source": [
    "### Task 3: GaussianNB with Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26accde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "print(\"GaussianNB Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "y_pred_dt = dtree.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143e0e0",
   "metadata": {},
   "source": [
    "## Part II: Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026259e",
   "metadata": {},
   "source": [
    "### Task 4: Conceptual Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29681753",
   "metadata": {},
   "outputs": [],
   "source": [
    "theory_dt = {\n",
    "    \"1. Entropy & Information Gain\":\n",
    "        \"Entropy is a measure of impurity. Information gain is the reduction in entropy after a dataset is split on a feature.\",\n",
    "    \"2. Gini vs Entropy\":\n",
    "        \"Gini is faster to compute and tends to isolate the most frequent class, while entropy is more theoretical and favors pure splits.\",\n",
    "    \"3. Overfitting in Decision Trees\":\n",
    "        \"Overfitting occurs when a tree is too deep. Avoid it by pruning, setting max depth, or using ensemble methods.\"\n",
    "}\n",
    "\n",
    "for q, a in theory_dt.items():\n",
    "    display(md(f\"**{q}**  \\n{a}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9eebf",
   "metadata": {},
   "source": [
    "\n",
    "### Task 5: Decision Tree on Titanic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd257a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Load dataset\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Preprocess\n",
    "df = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']]\n",
    "df.dropna(subset=['age'], inplace=True)\n",
    "df['sex'] = LabelEncoder().fit_transform(df['sex'])\n",
    "\n",
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "dt = DecisionTreeClassifier(max_depth=4)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 8))\n",
    "plot_tree(dt, feature_names=X.columns, class_names=['Died', 'Survived'], filled=True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, dt.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b258a7",
   "metadata": {},
   "source": [
    "### Task 6: Model Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c135b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "depths = range(1, 11)\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc.append(model.score(X_train, y_train))\n",
    "    test_acc.append(model.score(X_test, y_test))\n",
    "\n",
    "plt.plot(depths, train_acc, label=\"Train Accuracy\")\n",
    "plt.plot(depths, test_acc, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Overfitting Visualization\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831f7c0",
   "metadata": {},
   "source": [
    "## Part III: Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77715a92",
   "metadata": {},
   "source": [
    "### Task 7: Conceptual Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a4d0b",
   "metadata": {},
   "source": [
    "theory_ensemble = {\n",
    "    \"1. Bagging vs Boosting\":\n",
    "        \"Bagging trains multiple models independently on random subsets, while boosting trains sequentially to correct errors of previous models.\",\n",
    "    \"2. Random Forest Variance\":\n",
    "        \"Random Forest reduces variance by averaging multiple de-correlated decision trees trained on different data subsets.\",\n",
    "    \"3. Boosting Weakness\":\n",
    "        \"Boosting is sensitive to noisy data and can overfit if not properly regularized.\"\n",
    "}\n",
    "\n",
    "for q, a in theory_ensemble.items():\n",
    "    display(md(f\"**{q}**  \\n{a}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc18c9",
   "metadata": {},
   "source": [
    "### Task 8: Random Forest vs Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_rf = rf.predict(X_test)\n",
    "\n",
    "# Compare\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt.predict(X_test)))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_rf))\n",
    "print(\"Random Forest Precision:\", precision_score(y_test, y_rf))\n",
    "print(\"Random Forest Recall:\", recall_score(y_test, y_rf))\n",
    "\n",
    "# Feature Importance\n",
    "feat_imp = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "feat_imp.sort_values().plot(kind='barh', title='Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d34dc",
   "metadata": {},
   "source": [
    "### Task 9: Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "y_gb = gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_gb))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score:\", f1_score(y_test, y_gb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
